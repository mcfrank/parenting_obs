---
title             : "Understanding the impacts of video-guided activities on parent-child interaction"
shorttitle        : "Impacts of videos on parent-child interaction"

author: 
  - name          : "George Kachergis"
    corresponding: yes
    affiliation   : "1"
    address       : "Stanford, CA 94305 USA"
    email         : "kachergis@stanford.edu"
  - name          : "Emily Hembacher"
    affiliation   : "1"
  - name          : "Veronica Cristiano"
    affiliation   : "2"
  - name          : "Hanwen Vivian Zhang"
    affiliation   : "3"
  - name          : "Michael C. Frank"
    address       : "Stanford, CA 94305 USA"
    email         : "mcfrank@stanford.edu"
    affiliation   : "1"

authornote: |
  The data that support the findings of this study are openly available in OSF at http://doi.org/10.17605/OSF.IO/2BPDF

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution    : "Gallaudet University"
  - id            : "3"
    institution    : "Cornell University"

abstract: |
  Early parenting practices play an important role in shaping childrenâ€™s future outcomes. 
  In particular, high-quality early interactions can facilitate language learning and school performance. 
  The rise of phone-based parenting applications ("apps") could deliver low-cost interventions on parenting style to a wide variety of populations, especially the parents of very young children, who are often difficult to reach in other ways. 
  Yet little is known about the effects of communicating to parents through app-based interventions. 
  In two studies (one preregistered), we showed parents short videos depicting age-appropriate parent-child activities from a parenting app. 
  We found that after watching the video, parents spoke more and made more bids for joint attention, as compared with controls who watched no video (experiment 1) or a science video (experiment 2). 
  These results suggest that activity videos can lead to positive changes in parent engagement, providing support for the use of such videos in parenting interventions.
keywords          : "language development; parent intervention; childhood development; joint attention; lexical diversity"
wordcount         : "5066"
bibliography      : ["library.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
header-includes:
  - \usepackage{float}
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r global_options, include=FALSE}
# The data that support the findings of this study are openly available in OSF at https://osf.io/2bpdf/?view_only=d4b230deba2a4f91bdb2aba29088d982
#  The data that support the findings of this study are openly available in OSF at http://doi.org/10.17605/OSF.IO/2BPDF

knitr::opts_chunk$set(fig.width=4, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)
```

```{r setup, include = FALSE}
suppressPackageStartupMessages(c("dplyr","langcog","tidyr","ggplot2","lme4"))
library(papaja)
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(psych)
library(langcog)
library(tidyverse)
library(ggthemes)
library(lme4)
library(lmerTest)
library(tidyboot)
library(knitr)
library(rstanarm)
library(bayestestR)
source("multiplot.R")

select <- dplyr::select # masked by MASS
theme_set(theme_few())

# labels=addline_format(c("Activity Video", "Science Video"))
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}
```

```{r load-exp1-lexical-data, include=F}
e1path = "../parenting_obs_e1/"

d = read.csv("../language_analyses/exp1_lexical_diversity.csv") 

e1_parent_ed = table(d$parent_ed)

e1ld <- d %>%
  mutate(condition = factor(condition), 
         gender = as.factor(gender),
         video = as.factor(video))

e1_gender = table(e1ld$gender)

# rate calculations (tokens / min, words / min)
e1_play_duration <- read.csv(paste0(e1path,"joint_attention/parenting_obs_play_duration_E1.csv"), header=T)
e1_play_duration$start_sec = with(e1_play_duration, ifelse(is.na(verbalcue_start_sec), 
                                                           play_start_sec, verbalcue_start_sec))
e1_play_duration <- e1_play_duration %>% 
  mutate(play_duration = (play_stop_sec - start_sec) / 60) %>% # sec to mins
  select(SID, play_duration)
e1ld <- e1ld %>% left_join(e1_play_duration, by=c("sid"="SID"))



# convert types and tokens to rate: types/min and tokens/min
e1ld <- e1ld %>% 
  mutate(types = (types / play_duration), 
         tokens = (tokens / play_duration),
         TTRnorm = (TTR / play_duration), # # not really interpretable if we normalize LD measures
         MTLDnorm = (MTLD / play_duration)
         )

e1_6to12 = table(subset(e1ld, age<1)$condition) # con 9, exp 10
e1_12to18 = table(subset(e1ld, age<1.5 & age>=1)$condition) # con 11 exp 9
e1_18to24 = table(subset(e1ld, age>=1.5)$condition) # con 10, exp 11
```

```{r load-exp1-joint-attention-data, echo=F}
load(paste0(e1path,"joint_attention/Exp1_joint_attention_data.RData"))
d$Condition <- factor(d$Condition, levels = c("con","exp"), labels = c("No Video","Activity Video"))

e1ja <- d %>%
  #filter(!is.na(AA), !is.na(EL), !is.na(RR)) %>% # 6 parents NA -- don't want to remove from main analysis!
  mutate(Condition = factor(Condition), 
         bids_tot = as.numeric(bids_tot),
         EL = as.numeric(langcog::scale(EL, scale=FALSE)),
         AA = as.numeric(langcog::scale(AA, scale=FALSE)),
         RR = as.numeric(langcog::scale(RR, scale=FALSE)),
         age = as.numeric(langcog::scale(age, scale=FALSE)),
         parent_ed = langcog::scale(parent_ed, scale=TRUE),
         gender = as.factor(gender),
         Video = as.factor(Video)
         #pja_length = pja_length/1000,
         #cja_length = cja_length/1000,
         #total_lja = total_lja/1000
         ) # convert duration ms -> seconds
```

```{r load-exp2-lexical-data, echo=F}
e2path = "../parenting_obs_e2/"
load(paste(e2path,"Exp2_cleaned_data.RData",sep=''))
e2Age <- d %>% mutate(Age = age) %>%
  select(sid, Age)
e2_parent_ed = table(d$parent_ed) # unscaled parent_ed
d = read.csv("../language_analyses/exp2_lexical_diversity.csv")
d <- d %>% left_join(e2Age, by="sid")

e2ld <- d %>%
  mutate(condition = as.factor(condition), 
         gender = as.factor(gender),
         video = as.factor(video)) %>%
  select(-X.1, -X)

e2ld$Condition = factor(d$condition, levels = c("con", "exp"), labels = c("Science Video", "Activity Video"))

e2_gender = table(d$gender)
e2_12to18 = table(subset(d, Age<1.5)$Condition) # Activity Vid 20, Science Vid 21
e2_18to24 = table(subset(d, Age>=1.5)$Condition) # Activity Vid 22, Science Vid 21

#e2_dur = read.csv(paste0(e2path,"joint_attention/parenting_obs_play_duration_E2.csv"))
#e2_dur <- e2_dur %>% mutate(vid_dur=video_duration_sec / 60) %>% # sec / min
#  select(sid, vid_dur)
#e2ld <- e2ld %>% left_join(e2_dur, by="sid")

e2_play_duration <- read.csv(paste0(e2path,"joint_attention/parenting_obs_play_duration_E2.csv"), header=T)
e2_play_duration$start_sec = with(e2_play_duration, ifelse(is.na(verbalcue_start_sec), 
                                                           play_start_sec, verbalcue_start_sec))
e2_play_duration <- e2_play_duration %>% 
  mutate(play_duration = (play_stop_sec - start_sec) / 60) %>% # sec to min
  select(sid, play_duration)
e2ld <- e2ld %>% left_join(e2_play_duration, by="sid")

# convert types and tokens to rate: types/min and tokens/min
e2ld <- e2ld %>% 
  mutate(types = (types / play_duration), 
         tokens = (tokens / play_duration),
         TTRnorm = (TTR / play_duration), # not really interpretable if we normalize LD measures
         MTLDnorm = (MTLD / play_duration)
         )
```

```{r load-exp2-joint-attention-data, echo=F}
load(paste(e2path,"joint_attention/Exp2_joint_attention_data.RData",sep=''))
d$Condition <- factor(d$condition, levels = c("con","exp"), labels = c("Science Video","Activity Video"))
d$Age = d$age 
e2ja <- d %>% mutate(bids = bids,
                     age = as.numeric(langcog::scale(age, scale=FALSE))
                     #pja_length = pja_length/1000,
                     #cja_length = cja_length/1000,
                     #total_lja = total_lja/1000
                     ) # convert duration ms -> seconds

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


# Research Highlights 

* After watching short activity videos in a parenting app, parents made more attempts to engage with their children and spoke more to them.
* Older toddlers showed greater engagement after parents watched the activity videos, relative to younger toddlers.
* These activity videos led parents to set more goals for their child during play. 
* Our findings suggest that parent-led activities guided by apps may be helpful in scaffolding the play of older toddlers.

# Introduction

The quantity and quality of early language input has been found to be strongly associated with later language and academic outcomes [@Hart1995; @Marchman2008; @Rowe2012; @Cartmill2013; @HirshPasek2015; see @Schwab2016 for a review]. 
Thus, because of the potential for large downstream effects [@Heckman2006], there is tremendous interest in interventions that change children's language environment. 
And because parents define a large portion of that environment, especially before the onset of formal schooling, parent behavior is a critical locus for such interventions. 
Many effective parenting interventions require large resource investments and require many hours of in-person contact [@Jamaica2014; @PerryPreschool2004; @Leung2018], making implementation at scale a daunting proposition. 
For this reason, many researchers targeting early language are interested in delivering parenting interventions remotely -- through texts, apps, and videos delivered on digital devices. 
But what do parents take away from these short messages about what to do with or how to talk with their children?

The content provided by digital parenting interventions runs the gamut from general parenting messages and facts from child development research to specific advice, coaching, and suggested activities. 
A growing body of evidence suggests that these digital interventions can be effective across a range of cultures, income levels, and children's ages [for a review, see @Breitenstein2014].
For example, in contrast to a face-to-face parent training intervention, a tablet-based version saw significantly higher session completion rates (51% attendance vs. 85% module completion) and comparable or larger effect sizes on parents' and children's (aged 2 to 5 years) behavior [@Breitenstein2016].
Often, however, the theory of change presupposed by such interventions is relatively vague.
Both within and outside the realm of academic interventions, messages to parents of young children often seek to provide knowledge about some aspect of development (e.g., early language), often in tandem with a suggestion regarding activities.
Such messages are assumed to inform parents' choice of behaviors, spurring them to engage in some target activity, which is assumed to be more stimulating than what parents would have done otherwise. 

This theory of change is typically grounded in ideas about guided play and early language stimulation. 
Child-directed speech varies not only in quantity (i.e., the number of total tokens), but also in quality in terms of the diversity of the tokens [@Malvern2004] or the context-appropriateness of the speech [@Cartmill2013], both of which have been linked to children's subsequent language development.
Further, language learning---especially the acquisition of early vocabulary in the first years---appears to be supported preferentially by parents and children _jointly attending_ to some object or activity [@Baldwin1991; @Bigelow2004].
Episodes of joint attention are frequent during guided play, when parents set goals and scaffold their child's activities [@Wood1976; @Weisberg2013].
Thus, the current literature supports interventions that encourage parents to provide high-quality language and interaction through something like guided play---whether via reading books or playing with a shape-sorter at home, or via a conversation about categories in the supermarket.

But is this theory of change correct? That is, does the provision of knowledge and activities lead to higher-quality play? 
Alternatively, by focusing parents on a specific activity, this approach could be flawed, causing parents to over-focus on achieving the superficial goals of the activity. 
This problem might be especially likely with video messages, which could encourage parents to try to mimic a model's specific speech and/or actions.
Attempting to reproduce such surface details of a video-guided activity could in turn result in less high-quality talk, with less responsiveness to their child's play.
Another possibility is that these messages might produce the desired effect, but only for those parents who already have a general orientation towards children's early learning.

Our current experiments were designed to make a direct test of this question: How do parents change their interactions with young children on the basis of short video parenting messages? 
In two experiments, we collected a convenience sample of parent-child dyads at a local children's museum. 
We showed parents in the experimental group a single short video modeling an interactive toy-based activity along with a scientific justification. 
Parents in the control group received either no video (Experiment 1) or a video of a recent finding in developmental psychology (Experiment 2). 
We then gave the toys from the video to all dyads and videotaped their interactions, coding for caregivers' language quantity and quality as well as joint attention. 


# Experiment 1

In Experiment 1, we invited parents of 6- to 24-month-old infants visiting the Children's Discovery Museum in San Jose to complete video-guided activities from a commercial parenting app that delivers digital parenting advice in the form of short videos.
Parents were randomly assigned to one of two conditions: parents in the _Activity Video_ condition watched a video from the app (matched to their child's age), and then performed the activity with their child using the props from the video. 
Parents in the _No Video_ condition did not watch an activity video, but were given a set of the same age-appropriate props and asked to play with their infants as they normally would at home. 

## Method

### Participants
`r nrow(e1ld)` infants (F = `r e1_gender['F']`, M = `r e1_gender['M']`) aged 6-24 months (19 6-11.9 month-olds: 9 in the control condition, 10 in Activity Video; 20 12-17.9 month-olds: 11 control, 9 Activity Video; 21 18-24 month-olds: 10 control, 11 Activity Video) and their parents participated in a museum in northern California. 
We included infants who were exposed to English more than 50 percent of the time (n = 58) or who were exposed less but whose participating parent reported that they primarily speak English with their child at home (n = 2). 
62% of participants (n = 37) had been exposed to two or more languages, as indicated by their parent. 
Parents identified their children as White (n = 25), Asian (n = 11), African American/Black (n = 2), Biracial (n = 12), other (n = 5), or declined to state (n = 5). 
Fifteen parents reported that their child was of Hispanic origin. 
Parents tended to be highly-educated, with reports of highest level of education ranging from completed high school (n = `r e1_parent_ed[1]`), some college (n = `r e1_parent_ed[2]`), four-year college (n = `r e1_parent_ed[3]`), some graduate school (n = `r e1_parent_ed[4]`), to completed graduate school (n = `r e1_parent_ed[5]`).

### Materials
Stimuli included activity videos from a commercial parenting application. 
The videos were designed to show activities to parents that they could perform with their child in order to foster cognitive and physical development, and were targeted to the child's age and level of development. 
In each video, an adult and child perform the activity (e.g., sorting toys according to size) while a narrator explains the activity and its purpose. 
We selected two videos for each of three age groups in our sample (6-11.9 months, 12-17.9 months, 18-23.94 months).
Participants were also given a set of toys corresponding to those in the video that they watched so that they could complete the activity.[^1]

[^1]: Details of the specific videos used and the toys associated with each video are in the Appendix. 

Participants were randomly assigned to either the *Activity Video* condition or the *No Video* condition. 
Parents participating in the Activity Video condition were assigned to watch one of the two activity videos available for their child's age group, while parents in the No Video condition watched no video, and were simply asked to play with their child as they normally would.
The two conditions were yoked: for each Activity Video participant who saw a particular video and received the associated props, a participant in the No Video condition received the same props to use without seeing the video.
Parents also completed the Early Parenting Attitudes Questionnaire [EPAQ; @Hembacher2020]. 
The EPAQ measures parents of young children's attitudes about parenting and child development along three dimensions: rules and respect, early learning, and affection and attachment ([see SI](https://osf.io/2bpdf/?view_only=69c0c6ca5d2649438e8642a57671ba79)).

### Procedure
After providing informed consent, parents in the Activity Video condition watched the assigned activity video on a laptop with headphones.
To ensure that parents could give the video their full attention, the experimenter played with the infant with a set of toys (different from the experimental props used in the study) while the video was being played. 
Immediately following the video, each parent-child dyad was provided with the props to complete the video-guided activity that the parent had viewed. 
The toys were placed on a large foam play mat, and parents were instructed to sit on the mat with their child and re-create the activity they had viewed for a period of approximately three minutes.[^2]
In the No Video condition, after informed consent parents were told to play with their child as they would at home with the provided props for a period of three minutes. 
They were not given any additional instructions about how to use the props.

[^2]: Based on piloting, we estimated these activities would would only require three minutes to complete.

In both conditions, two video cameras were used to record the play session from different angles, and parents were fitted with a wireless Shure lavalier microphone to record their child-directed speech. 
After three minutes of play had elapsed, parents were told they could stop playing and the cameras and microphone were turned off.
Parents were then asked to complete the EPAQ before being debriefed. 


### Joint Attention Coding Procedure
The video of each session was manually coded for episodes of joint attention (JA) using the Datavyu software [@datavyu], and coders were blind to condition. 
The video taken at floor level was coded by default, but the other video was referred to if the participants were occluded or if there was technical difficulty with the first camera. 
Each session's video was coded for episodes of coordinated JA, episodes of passive JA, and parental bids for JA. 
Parental bids for JA were defined as any attempt to initiate joint attention (i.e labeling, pointing, or otherwise drawing attention to an object) that did not result in passive or coordinated JA. 
If more than 3 seconds elapsed between bids, they were coded as separate attempts. 
An episode of joint attention was considered _passive_ if both participants visually focused on an object for 3 or more seconds but the child did not acknowledge the parent. 
If either participant looked away from the object for less than 3 seconds and then returned to the same object it was considered part of the same episode of joint attention. 
A joint attention episode was considered _coordinated_ if both participants visually focused on an object for 3 or more seconds and at some point in the interaction the child indicated awareness of interaction with some overt behavior toward the parent such as looking at their face, gesturing, vocalizing, or turn-taking. 
Full details of our guidelines for coding joint attention are available in SI.

A second coder independently coded a third of the videos (i.e., 20 of the 60 videos, approximately equally distributed across ages) to establish reliability. 
The two coders had a reliability of ICC = 0.79 with 95% confidence interval (CI) = [0.55, 0.91] for rate of parent bids for JA (number of bids per minute); ICC = 0.34 with 95% CI = [-0.11, 0.67] for rate of passive JA episodes (per minute); ICC = 0.66 with 95% CI = [0.32, 0.85] for rate of coordinated JA episodes; ICC = 0.33 with 95% CI = [-0.11, 0.67] for time spent (seconds per minute) in passive JA episodes, and ICC = 0.60 with 95% CI = [0.24, 0.82] for time spent in coordinated JA episodes. 


## Results

Parents' child-directed speech during the play sessions (mean duration: `r round(mean(e1ld$play_duration), 2)` min) was transcribed; child utterances were not considered.
Although Experiment 1 was not preregistered, for consistency the transcripts and hand-coded joint attention data were analyzed according to our preregistration for Experiment 2[^3], with any deviations or exploratory analyses noted.
Below we first report the lexical diversity results, followed by the joint attention results.

[^3]: Preregistration: [https://osf.io/6k9m8/](https://osf.io/6k9m8/?view_only=d4b230deba2a4f91bdb2aba29088d982)

### Lexical Diversity

For each transcript of child-directed speech, the caregivers' unique word *types* and *tokens* (total words) uttered were tallied and converted to rates (e.g., tokens per minute of play), and the type-token ratio (TTR) was calculated as a measure of lexical diversity. 
Although we initially preregistered TTR as our measure of lexical diversity (since it is a simple, commonly used measure), it has been noted that TTR is correlated with the length of a text, which has led to the development of new measures such as the measure of textual lexical diversity [MTLD; @McCarthy2010]. 
Thus, we also measure lexical diversity with MTLD, which is calculated as the mean length of sequential word strings in a text that maintain a given TTR value (we use 0.72, as proposed by McCarthy & Jarvis).


```{r, e1-lexdiv-regressions, echo=F, include=F}
# get estimated coefficient and 89% CI from Bayesian regression
get_stan_glmer_reporting_values <- function(model, cond, digits=2) {
  ci95 <- posterior_interval(model, prob = 0.95, pars = cond)
  ci89 <- posterior_interval(model, prob = 0.89, pars = cond) # 89% CIs as recommended by Kruschke and others
  pd = p_direction(model)
  pd = pd[which(pd$Parameter==cond),]$pd
  ci = round(c(ci95,ci89,pd), digits)
  names(ci) = c("95pLB","95pUB","89pLB","89pUB","pd") # 95% and 89% lower and upper bounds
  beta = round(model$coefficients[cond], digits)
  return(c(beta,ci))
}

print_CI <- function(mod) {
  string = paste0("89% CI = [",mod['89pLB'],", ",mod['89pUB'],"]", ", *pd* = ",mod['pd'])
  return(string)
}

modTTR <- stan_glmer(TTR ~ condition * age + (1|video), data=e1ld, adapt_delta = .97)
summary(modTTR) # ESS from ~1800 - 3600
p_direction(modTTR, effects="all") # condition
e1TTR = get_stan_glmer_reporting_values(modTTR, "conditionexp") # -.11

modMTLD <- stan_glmer(MTLD ~ condition * age + (1|video), data=e1ld)
summary(modMTLD) # ESS ~ [2200, 4100]
p_direction(modMTLD, effects="all") # condition
e1MTLD = get_stan_glmer_reporting_values(modMTLD, "conditionexp") # -8.48


# predict word tokens
tokens_mod <- stan_glmer(tokens ~ condition * age + (1|video), control = list(adapt_delta = 0.99), data=e1ld) 
summary(tokens_mod)
p_direction(tokens_mod, effects="all") # condition
e1tokens = get_stan_glmer_reporting_values(tokens_mod, "conditionexp") # 19.45

# predict word types
types_mod <- stan_glmer(types ~ condition * age + (1|video), control = list(adapt_delta = 0.99), data=e1ld,
                        adapt_delta = .97)
summary(types_mod) 
p_direction(types_mod, effects="all") # cage 78, cond*age 83%

e1_lexdiv <- e1ld %>% group_by(condition) %>%
  summarise(TTR=mean(TTR), MTLD=mean(MTLD), tokens=mean(tokens)) # types=mean(types) # con=16.7, exp=18.1
```

```{r, e1ld-exploratory, echo=F, include=F}
# Predicting lexical diversity based on experimental condition, PAQ, demographics. 
e1model_string = "~ condition*EL + condition*AA + condition*RR  + age + gender + parent_ed + (1|video)"
# only 54 parents completed the EPAQ

modTTRE <- stan_glmer(paste("TTR", e1model_string), data=e1ld)
summary(modTTRE) 
p_direction(modTTRE, effects="all") # RR 87%  parent_ed 90.9%

modMTLDE <- stan_glmer(paste("MTLD", e1model_string), data=e1ld)
summary(modMTLDE) #
p_direction(modMTLDE, effects="all") # genderM 85%, RR 85%, AA 77%..

tokens_modE <- stan_glmer(paste("tokens", e1model_string),  data=e1ld) 
summary(tokens_modE) # condition 54.4 
get_stan_glmer_reporting_values(tokens_modE, 'parent_ed') # 
p_direction(tokens_modE, effects="all") # RR 73.5% parent_ed 90.5%

# predict word types
types_modE <- stan_glmer(paste("types", e1model_string), data=e1ld) 
summary(types_modE) 
p_direction(types_modE, effects="all") # EL 84% AA 79% genderM 79%, parent_ed 89% cond:AA 96.3%
e1types_condAA = get_stan_glmer_reporting_values(types_modE, 'conditionexp:AA') # ADD THIS TO TEXT

```

We fit a Bayesian mixed-effects linear regression predicting TTR as a function of condition, age (centered), and their interaction with a random intercept per video using rstanarm [@rstanarm]. 
For effects that are at least 95% likely to be non-zero according to the posterior distribution (i.e., probability of direction: *pd*; @Makowski2019), we report estimated coefficients ($\beta$) as well as 89% Bayesian credible intervals (89% CI)[^4], demarcating the range within which 89% of the posterior falls, meaning that given the observed data, the effect has 89% probability of falling within this range (see Appendix for more an illustrated explanation).
There was lower TTR in the Activity Video condition (mean: `r round(subset(e1_lexdiv, condition=='exp')$TTR, 2)`) than in the No Video condition (mean: `r round(subset(e1_lexdiv, condition=='con')$TTR, 2)`, $\beta=$`r e1TTR["conditionexp"]`, `r print_CI(e1TTR)`). 
A similar regression instead predicting MTLD also found lower lexical diversity in the Activity Video condition (mean MTLD: `r round(subset(e1_lexdiv, condition=='exp')$MTLD, 2)`) than in the No Video condition (mean: `r round(subset(e1_lexdiv, condition=='con')$MTLD, 2)`, $\beta=$`r e1MTLD["conditionexp"]`, `r print_CI(e1MTLD)`), with no notable influence of age.
Figure \ref{fig:lexdiv} (top) shows the mean of each lexical diversity measure (TTR and MTLD) by condition.

[^4]: 89% CIs are recommended for Bayesian analyses because unless the effective sample size (ESS) is on the order of 10,000, the 95% credible interval is unstable [@Kruschke2014; @McElreath2018; @bayestestR]. Our ESS ranges from 2,000-5,000, and thus we report the tighter, more stable interval.

```{r e1tokens, echo=F, include=F}
e1ld$Condition = factor(e1ld$condition, levels = c("con", "exp"), labels = c("No Video", "Activity Video"))

e1_mtld <- e1ld %>% group_by(Condition) %>%
  tidyboot_mean(MTLD) 
e1_lex <- e1ld %>% group_by(Condition) %>%
  tidyboot_mean(TTR) # Type/Token Ratio
e1_tok <- e1ld %>% group_by(Condition) %>%
  tidyboot_mean(tokens) 
e1_type <- e1ld %>% group_by(Condition) %>%
  tidyboot_mean(types)
```

We also conducted similar regressions predicting the rate of word tokens and types per minute of play, finding a notable effect of condition on the rate of word tokens ($\beta=$ `r e1tokens["conditionexp"]`, `r print_CI(e1tokens)`), with parents using tokens at a higher rate in the Activity Video condition (mean: `r round(subset(e1_lexdiv, condition=='exp')$tokens, 0)` tokens/min, bootstrapped 95% confidence interval (conf. int.): [`r round(subset(e1_tok, Condition=="Activity Video")$ci_lower, 0)`, `r round(subset(e1_tok, Condition=="Activity Video")$ci_upper, 0)`]) than in the No Video condition (mean: `r round(subset(e1_lexdiv, condition=='con')$tokens, 0)`, 95% conf. int.: [`r round(subset(e1_tok, Condition=="No Video")$ci_lower, 0)`, `r round(subset(e1_tok, Condition=="No Video")$ci_upper, 0)`]).


```{r e1tab, echo=F, include=F}
e1 <- e1ld %>% group_by(Condition) %>%
  summarise("Mean TTR"=mean(TTR), TTRsd=sd(TTR),
            "Mean MTLD"=mean(MTLD), MTLDsd=sd(MTLD),
            "Mean types"=mean(types), types.sd=sd(types),
            "Mean tokens"= mean(tokens), tokens.sd=sd(tokens))
names(e1) = c("Condition","TTR/min (M)", "(sd)", "MTLD/min (M)", "(sd)", "Types/min (M)", "(sd)", "Tokens/min (M)", "(sd)")
kable(e1, digits=2, caption="Lexical diversity measures in Experiment 1.")
```

```{r e1ld-figs, include=F}
small_text = theme(
  plot.title = element_text(size = 12, face="bold"),
  axis.title.x = element_text(size = 10),
  axis.text.x = element_text(size = 8),
  axis.title.y = element_text(size = 10),
  axis.text.y = element_text(size = 8))

e1ttr <- ggplot(e1_lex, aes(x=Condition, y=mean, fill=Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .7)) +
  geom_point(data=e1ld, aes(x=Condition, y=TTR), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Type/Token Ratio (TTR)") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") + ggtitle("Experiment 1") + small_text
# want carriage return in Condition labels..

e1mtld <- ggplot(e1_mtld, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e1ld, aes(x=Condition, y=MTLD), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("MTLD") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") + ggtitle(" ") + small_text

e1tokens <- ggplot(e1_tok, aes(x=Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e1ld, aes(x=Condition, y=tokens), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Mean Word Tokens per Minute") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") + ggtitle(" ") + small_text

```


```{r fig-lexdiv, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=6.5, fig.height=5.5, fig.cap = "\\label{fig:lexdiv} Mean lexical diversity scores (left: Type/Token ratio, middle: MTLD) and mean number of tokens used by condition (right) in Experiment 1 (top) and Experiment 2 (bottom). Error bars show bootstrapped 95\\% confidence intervals, and gray dots indicate values for each participant."}

ttr2 <- e2ld %>% group_by(Condition) %>%
  tidyboot_mean(TTR) 

e2ttr <- ggplot(ttr2, aes(x=Condition, y=mean, fill=Condition)) + geom_bar(stat="identity") + # labels=addline_format(c("Science Video", "Activity Video"))
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e2ld, aes(x=Condition, y=TTR), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Type/Token Ratio (TTR)") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") + ggtitle("Experiment 2") + small_text

mtld2 <- e2ld %>% group_by(Condition) %>%
  tidyboot_mean(MTLD) 

e2mtld <- ggplot(mtld2, aes(x = Condition, y = mean, fill = Condition)) + 
  geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e2ld, aes(x=Condition, y=MTLD), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("MTLD") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") + ggtitle(" ") + small_text

e2tok <- e2ld %>% group_by(Condition) %>%
  tidyboot_mean(tokens) 

e2tokens <- ggplot(e2tok, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e2ld, aes(x=Condition, y=tokens), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Mean Word Tokens per Minute") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") + ggtitle(" ") + small_text

#multiplot(e1ttr, e1mtld, e1tokens, cols=3) # Exp1 LD graph
#multiplot(e2ttr, e2mtld, e2tokens, cols=3) # Exp2 LD graph

multiplot(e1ttr, e1mtld, e1tokens, 
          e2ttr, e2mtld, e2tokens, cols=3, layout=matrix(c(1,2,3, 4,5,6), nrow=2, byrow=T)) 
```


```{r, e1-effect-size, echo=F, include=F}
cohens_d <- function(dat) {
  return( diff(dat$mean) / (sqrt(sum(dat$sd^2)) / 2) )
}

ttr_means <- e1ld %>% group_by(Condition) %>%
  summarise(mean = mean(TTR), sd = sd(TTR))
cohens_d(ttr_means) # -1.68 - should we report this?
```

### Joint Attention

```{r, e1ja-regressions, echo=F, include=F}

model_string = "~ Condition * age + (1| Video)" # + gender + parent_ed
# Total number of bids
e1bids_mod <- stan_glmer(paste("bids_tot", model_string), data = e1ja, adapt_delta = .98) # 
summary(e1bids_mod) 
p_direction(e1bids_mod, effects="all") # condition
e1bids = get_stan_glmer_reporting_values(e1bids_mod, "ConditionActivity Video") # .72

e1bids_M <- e1ja %>% group_by(Condition) %>% 
  summarise(mean = mean(bids), sd = sd(bids)) 


# Episodes of passive joint attention.
e1pja_mod <- stan_lmer(paste("pja", model_string), data = e1ja)
summary(e1pja_mod) # NA
p_direction(e1pja_mod, effects="all") # condition 89% 


# Episodes of coordinated joint attention.
e1cja_mod <- stan_lmer(paste("cja", model_string), adapt_delta = 0.99, data = e1ja) 
summary(e1cja_mod) # NA
p_direction(e1cja_mod, effects="all") # condition 81%


# Total duration of passive joint attention.
e1pja_time_mod <- stan_lmer(paste("pja_length", model_string), data = e1ja, adapt_delta=.98)
summary(e1pja_time_mod) # NA
p_direction(e1pja_time_mod, effects="all") # condition 90.6%


# Total duration of coordinated joint attention.
e1cja_time_mod <- stan_lmer(paste("cja_length", model_string), data = e1ja)
summary(e1cja_time_mod) # NA
p_direction(e1cja_time_mod, effects="all") # cond*age 86%
```

We fit a Bayesian mixed-effects linear regression predicting the rate of parent bids (per minute) for joint attention (JA) as a function of fixed effects of condition, age (centered), and their interaction, with random intercepts per video. 
Parents' bid rate was greater in the Activity Video condition (mean: `r round(subset(e1bids_M, Condition=="Activity Video")$mean,2)`, sd: `r round(subset(e1bids_M, Condition=="Activity Video")$sd,2)`) than in the No Video condition (mean: `r round(subset(e1bids_M, Condition=="No Video")$mean,2)`, sd: `r round(subset(e1bids_M, Condition=="No Video")$sd,2)`, $\beta=`r e1bids["ConditionActivity Video"]`$, `r print_CI(e1bids)`). 
Mixed-effects regressions with the same structure were performed predicting the rate of episodes of coordinated and passive JA, and the time spent in coordinated and passive JA.
There were no notable effects on the rate of episodes nor on the time spent in coordinated or passive JA episodes.
Figure \ref{fig:JA} (top) shows the mean rate of bids and episodes of JA by condition in Experiment 1.


### Exploratory Analyses

```{r e1ja-epaq-regressions, include=F, echo=F}
# follow-up regression (if we find a sig effect?)
model_stringE =  "~ Condition * EL + Condition * AA + Condition * RR  + age + gender + parent_ed + (1| Video)"

# Rate of parental bids
e1bids_modE <- stan_glmer(paste("bids_tot", model_stringE), data = e1ja) 
summary(e1bids_modE) # cond, gender 
p_direction(e1bids_modE, effects="all")
e1bidsE = get_stan_glmer_reporting_values(e1bids_modE, "genderM") # .73 [.11,1.36] REPORT 

e1bidsE_M <- e1ja %>% group_by(gender) %>% 
  summarise(mean = mean(bids), sd = sd(bids)) 

# Episodes of coordinated joint attention.
e1cja_modE <- stan_lmer(paste("cja", model_stringE), data = e1ja)
p_direction(e1cja_modE, effects="all") # RR 89% parent_ed 89%
summary(e1cja_modE) # NA

# Episodes of passive joint attention.
e1pja_modE <- stan_lmer(paste("pja", model_stringE), data = e1ja)
summary(e1pja_modE) # condition*RR beta=1.8, RR close
p_direction(e1pja_modE, effects="all") # RR 98%, parent_ed 98%
e1pjaRR = get_stan_glmer_reporting_values(e1pja_modE, "RR") # -.34, 89% CI = [-.65,-.02]  
e1pjaE = get_stan_glmer_reporting_values(e1pja_modE, "ConditionActivity Video:RR") # .60 [.14,1.05]
e1pja_parent_ed = get_stan_glmer_reporting_values(e1pja_modE, "parent_ed") 

# Total duration of passive joint attention.
e1pja_time_modE <- stan_lmer(paste("pja_length", model_stringE), data = e1ja)
summary(e1pja_time_modE) # NA
p_direction(e1pja_time_mod, effects="all") # condition 91%

# Total duration of coordinated joint attention.
e1cja_time_modE <- stan_lmer(paste("cja_length", model_stringE), data = e1ja)
summary(e1cja_time_modE) # NA
p_direction(e1cja_time_mod, effects="all") # condition:age 86%
```


We also fit Bayesian mixed-effects linear regression models predicting each of the above lexical diversity and joint attention dependent variables as a function of fixed effects of condition, age (centered), the child's sex, parent's education level, and the subscales of the EPAQ: Early Learning (EL), Affection and Attachment (AA), and Rules and Respect (RR), along with interactions of condition and EL, AA, and RR. 
These models included random intercepts per video. 
Of these exploratory regressions, two showed notable effects involving the EPAQ subscales, and one other showed an effect of the child's sex.
First, in the word types regression, parents with higher Affection and Attachment scores used more word types per minute after watching an Activity Video (interaction of condition and AA: $\beta=`r e1types_condAA["conditionexp:AA"]`$, `r print_CI(e1types_condAA)`).
Second, in the regression examining the rate of passive JA episodes, parents scoring higher on the Rules and Respect (RR) subscale had a lower rate of passive JA episodes ($\beta=`r e1pjaRR["RR"]`$, `r print_CI(e1pjaRR)`). 
However, after an Activity Video, higher RR parents did not show this decrease in passive JA (interaction of condition and RR:  $\beta=`r e1pjaE["ConditionActivity Video:RR"]`$, `r print_CI(e1pjaE)`). 
Parents with higher education also showed a lower rate of passive JA episodes ($\beta=`r e1pja_parent_ed['parent_ed']`$, `r print_CI(e1pja_parent_ed)`).
Finally, parents with male children made bids for JA at a higher rate (mean: `r round(e1bidsE_M[2,]$mean,2)` bids/min) than those with female children (mean: `r round(e1bidsE_M[1,]$mean,2)`, $\beta=`r e1bidsE["genderM"]`$, `r print_CI(e1bidsE)`).

To get a better sense of the intervention's effect on language use, we analyzed which words were characteristic of parents' speech in each condition, comparing the difference in frequency rank of each word (lemma) in the two conditions, as well as contrasting the corpus overall with a general English-language word frequency list ([see SI](https://osf.io/2bpdf/files/github/language_analyses/Exp1_characteristic_chart.html) for the interactive corpus characteristic plot). 
Words that were strongly indicative of being from the Activity Video condition include "give", "big", "small", "ribbit", "thank", "have", and "bus", while words that were most characteristic of the No Video condition include "shake", "ready", "oh", "on", "going", "going", "see", "let", and the child's name. 

```{r fig-JA, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=6.5, fig.height=5.5, fig.cap = "\\label{fig:JA} Mean number of bids (left) and episodes (right) of joint attention (JA) by condition in Experiment 1 (top). For Experiment 2 (bottom), mean number of bids for JA by condition (left) and the number of episodes of JA by age and condition (right)."}
# Parents made significantly more bids for JA after watching an activity video, but this did not result in a greater number of episodes of JA.

ms_bids1 <- e1ja %>% group_by(Condition) %>%
  tidyboot_mean(bids_tot) 

e1bids <- ggplot(ms_bids1, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e1ja, aes(x=Condition, y=bids_tot), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Parental Bids for JA per Minute") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") + ggtitle("Experiment 1") + small_text

# total joint attention episodes
ms_tja1 <- e1ja %>% group_by(Condition) %>%
  tidyboot_mean(total_ja) 

e1tja <- ggplot(ms_tja1, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e1ja, aes(x=Condition, y=total_ja), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Episodes of JA per Minute") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") + ggtitle(" ") + small_text
  

e2_cja <- e2ja %>% group_by(Condition) %>%
  tidyboot_mean(cja)

library("scales")
#show_col(solarized_pal()(2)) # blue, red
sol_colors = solarized_pal()(2)

e2_bids <- e2ja %>% group_by(Condition) %>%
  tidyboot_mean(bids)
e2bids <- ggplot(e2_bids, aes(x = Condition, y = mean, fill = str_wrap(Condition, 8))) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  labs(fill="Condition") + 
  geom_point(data=e2ja, aes(x=Condition, y=bids), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Parental Bids for JA per Minute") + 
  scale_fill_manual(values=sol_colors) + ggthemes::theme_few() + ggtitle("Experiment 2") + small_text +
  theme(legend.position="none")
  #theme(legend.key.height=unit(1.2, "cm"), legend.text=element_text(size=9))

# total JA episodes (not currently graphed)
e2_tja <- e2ja %>% group_by(Condition) %>%
  tidyboot_mean(total_ja) 
e2tja <- ggplot(e2_tja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e2ja, aes(x=Condition, y=total_ja), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Episodes of JA per Minute") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  ggtitle(" ") + small_text + theme(legend.position="none")

e2tja_age = ggplot(e2ja, aes(x = Age, y = total_ja, group=Condition, colour=Condition)) +
  geom_point(alpha=.8) + geom_smooth(method='lm') + scale_colour_manual(values=rev(sol_colors)) +
  xlab("Age (years)") + ylab("Episodes of JA per Minute") +
  ggthemes::theme_few() + ggtitle(" ") + small_text + 
  theme(legend.position="bottom", legend.text=element_text(size=9), 
        legend.margin=margin(0,0,5,0),
        legend.box.margin=margin(-10,-10,-5,-10))
  #theme(legend.position="none")

# multiplot(e1bids, e1tja, cols=2) # old Exp1
# multiplot(e2bids, e2tja_age, cols=5, layout=matrix(c(1,1,2,2,2), nrow=1))
multiplot(e1bids, e1tja, e2bids, e2tja_age, 
          cols=6, layout=matrix(c(1,1,1,2,2,2, 3,3,4,4,4,4), nrow=2, byrow=T)) 
```


### Discussion

In summary, while parents produced more word types and tokens after viewing the activity video, lexical diversity (both TTR and MTLD) was higher when parents were just asked to play as they normally would.
It may be that parents in the Activity Video condition, in their attempt to stick to the prescribed task, end up repeating themselves more, and indeed some differences in speech acts were notable: 
after the Activity Video, parents used more words related to requests (e.g., "Can I have X? / Give me X. Thank you!"), whereas after no intervention parents' language related more to invitations (e.g., "Are you ready?" / "Let's see.").
However, parents who watched an activity video also made more bids for JA with their child.
This did not result in a greater number of successful episodes of JA---passive or coordinated---than dyads in the No Video condition, although low reliability in passive JA coding (which led us to refine our JA coding guidelines for Experiment 2) may limit our ability to detect an effect there.
In sum, the results of Experiment 1 suggest that digital parenting advice can increase parents' efforts to engage their child in joint attention, expand the volume if not diversity of their speech, and can shift the type of speech acts towards more requests.
<!-- discuss exploratory regression results? -->

# Experiment 2

Experiment 1 found that parents who watched an activity video made more bids for joint attention and spoke more words tokens per minute to their children, but had lower lexical diversity compared to parents who played with their children as they normally would at home. 
Might it be that parents who are focused on a specific activity show reduced lexical diversity due to their focus on engaging their child in the activity?
Experiment 2 focuses on replicating the key findings using a stronger control group, as well as a restricted number of preregistered predictions.[^5]

[^5]: Preregistration: [https://osf.io/6k9m8/](https://osf.io/6k9m8/?view_only=d4b230deba2a4f91bdb2aba29088d982). 

## Method

### Participants
`r nrow(e2ld)` infants (F = `r e2_gender['F']`, M = `r e2_gender['M']`) aged 12-24 months 
(`r e2_12to18["Activity Video"]` 12-17.9 month-olds in the Activity Video condition; 
`r e2_12to18["Science Video"]` 12-17.9 month-olds in the Science Video condition; 
`r e2_18to24["Activity Video"]` 18-24 month-olds in the Activity Video condition;
`r e2_18to24["Science Video"]` 18-24 month-olds in the Science Video condition) and their parents participated in the same museum as Experiment 1. 
We included infants who were exposed to English more than 50 percent of the time or who were exposed less but whose participating parent reported that they primarily speak English with their child at home. 
Forty-nine percent of participants (n = 41) had been exposed to two or more languages as indicated by their parent. 
Parents identified their children as White (n = 39), Asian (n = 20), African American/Black (n = 1), Biracial (n = 9), other (n = 7), or declined to state (n = 8). 
Sixteen parents reported their child was of Hispanic origin. 
Parents tended to be highly-educated, with reports of highest level of education ranging from some college (n = `r e2_parent_ed["3"]`), four-year college (n = `r e2_parent_ed["4"]`), some graduate school (n = `r e2_parent_ed["5"]`), to completed graduate school (n = `r e2_parent_ed["6"]`) or declined to state (n = 13).

### Materials
The design of Experiment 2 was the same as that of Experiment 1, except that instead of seeing no video in the control condition, parents instead watched a video that was generally related to child development research, but did not give any specific instructions about how to interact with infants or children. 
This condition was included to control for the possibility that differences in language output and joint attention in Experiment 1 could be due to simply cueing parents to think about infants' learning and cognitive development. 
The videos presented in the Control Video condition were media clips (available on YouTube) of developmental psychologists explaining their research interleaved with footage of infants or toddlers engaged in developmental research studies. 
Thus, the content of the videos superficially matched those in the Activity Video condition, but did not suggest any particular activities.
The videos were trimmed to approximately match the average video length in the Activity Video condition (close to 90 s).
Details of the videos used in the Activity Video conditions are in the Appendix.

### Procedure
The procedure for Experiment 2 matched that of Experiment 1, except that parents randomly-assigned to the Control Video condition watched a control video before the play session. 
Consistent with the No-Video control condition in Experiment 1, parents in the Control Video condition were told to play with their child as they would at home, and were not given additional instructions.
The coding procedure also matched that of Experiment 1. 
To establish reliability a second coder independently coded 25 of the 84 videos, approximately equally distributed across ages. 
The two coders had a reliability of ICC = 0.81 with 95% confidence interval (CI) = [0.62,0.91] for number of parent bids for JA; ICC = 0.74 with 95% CI = [0.48, 0.88] for number of passive JA episodes; ICC = 0.80 with 95% CI = [0.61, 0.91] for number of coordinated JA episodes; ICC = 0.72 with 95% CI = [0.44, 0.86] for time spent (secs/min) in passive JA episodes, and ICC = 0.88 with 95% CI = [0.75, 0.94] for time spent in coordinated JA episodes. 


## Results

Parents' child-directed speech during the play sessions (mean duration: `r round(mean(e2ld$play_duration), 2)` min) was transcribed and processed, and bids and episodes of joint attention were coded according to the same procedure used in Experiment 1.
We first report preregistered regressions[^6] predicting TTR and rate of word tokens, as well as an exploratory regression predicting MTLD.
We then turn to preregistered regressions of parental bids for joint attention and the total number of JA episodes.

[^6]: Although the preregistration implied the use of standard linear mixed-effects regression through the specification of adopting an alpha level of .005 for statistical significance, the non-convergence of some regressions led us to switch to Bayesian regression. Using a Bayesian analysis has the added benefit of not requiring arbitrary changes to alpha levels to correct for multiple comparisons (Gelman, 2008).

### Lexical Diversity
```{r, e2-lexdiv-regressions, echo=F, include=F}
# preregistered analyses:
# The number of parent bids for joint attention, the number of episodes of joint attention, the number of word tokens produced by parents, and the lexical diversity of parent language output (word types/word tokens). 
# 1) bids, 2) total JA eps, 3) # tokens 4) TTR 
# Mixed-effects linear regressions predicting the dependent variables with condition (video scaffolding vs. control) and age in months as fixed effects and participant and specific video as random effects. 
# DV ~ age * condition + (1 | participant) + (1 | video)
# Lab standard operating procedures will be used for random effects pruning in case of non-convergence.
# We will interpret condition main effects as well as age * condition interactions (positive or negative) as evidence for behavior changes caused by video scaffolding. 
# Because of the larger number of coefficients of interest (4 models x 2 coefficients), we will adopt an alpha level of .005 for statistical significance and will report alpha between .05 and .005 as suggestive.
# "secondary analyses"
# We will analyze the proportion of coordinated vs. passive joint attention separately using the same model specification.

# Notes: 
# - including (1|sid) never works: "Error: number of levels of each grouping factor must be < number of observations", so we back off to (1|video)
# Predicting lexical diversity based on experimental condition. 
e2TTR_m <- stan_lmer(TTR ~ age * condition + (1 | video), data=e2ld) 
summary(e2TTR_m) 
p_direction(e2TTR_m, effects="all") # condition, age 85%
e2TTR <- get_stan_glmer_reporting_values(e2TTR_m,"conditionexp") # -.09 [-.15,-.04]


# predict word tokens
e2tokens_m <- stan_lmer(log(tokens) ~ age * condition + (1 | video), data=e2ld)
summary(e2tokens_m) # NA 
p_direction(e2tokens_m, effects="all") # condition 90%
#launch_shinystan(e2tokens_m, ppd = FALSE)
#summary(e2tokens_m, 
#        pars = c("(Intercept)", "age", "conditionexp", "age:conditionexp"),
#        probs = c(0.025, 0.975),
#        digits = 2)
#plot(e2tokens_m)
#prior_summary(object = e2tokens_m)
get_stan_glmer_reporting_values(e2tokens_m,"conditionexp") 

# require(bayesplot)
# bayesplot_grid(
#   plot(e2tokens_m, plotfun = "intervals", pars = c("(Intercept)", "age", "conditionexp", "age:conditionexp"), prob = 0.5, prob_outer = 0.89, point_est = "median") + ggtitle("Marginal Posterior Parameter Estimates"),
#   plot(e2tokens_m, plotfun = "areas", pars = c("(Intercept)", "age", "conditionexp", "age:conditionexp"), prob = 0.5, prob_outer = 0.89, point_est = "median") + ggtitle("Marginal Posterior Parameter Estimates"),
#   grid_args = list(ncol = 2)
# )


e2MTLD_m <- stan_lmer(MTLD ~ age * condition + (1 | video), data=e2ld)
summary(e2MTLD_m) # NA
p_direction(e2MTLD_m, effects="all") # cond 76% age:cond 83%

e2_lexdiv <- e2ld %>% group_by(condition) %>%
  summarise(TTR=mean(TTR), MTLD=mean(MTLD), tokens=mean(tokens)) 
```

We fit a Bayesian mixed-effects linear regression predicting TTR as a function of age (centered) and condition with an interaction term, and with random intercepts per video.
This revealed lower TTR after the Activity Video (mean: `r round(e2_lexdiv[2,]$TTR, 2)`) than after the Science Video (mean: `r round(e2_lexdiv[1,]$TTR, 2)`, $\beta=`r e2TTR["conditionexp"]`$, , `r print_CI(e2TTR)`). 
The preregistered regression predicting the number of tokens used by parents revealed no effects. 
An exploratory mixed-effects linear regression predicting MTLD found no effect of age or condition.
Figure \ref{fig:lexdiv} (bottom left and middle) shows the mean of each lexical diversity measure (TTR and MTLD) by condition.
Regressions with the same structure predicting the number of words tokens found no effect of age or condition.
The means of the lexical measures are shown in Table 1.


```{r e2tab, echo=F}
e2 <- e2ld %>% group_by(Condition) %>%
  summarise("Mean TTR"=mean(TTR), TTRsd=sd(TTR),
            "Mean MTLD"=mean(MTLD), MTLDsd=sd(MTLD),
            "Mean types"=mean(types), types.sd=sd(types),
            "Mean tokens"= mean(tokens), tokens.sd=sd(tokens))
names(e2) = c("Condition","TTR", "(sd)", "MTLD", "(sd)", "Types", "(sd)", "Tokens", "(sd)")
#kable(e2, digits=2, caption="\\label{e2tab} Lexical diversity measures in Experiment 2.")
apa_table(e2, caption = "Mean lexical diversity measures in Experiment 2.")
```



```{r, e2-effect-size, echo=F, include=F}
ttr_means <- e2ld %>% group_by(Condition) %>%
  summarise(mean = mean(TTR), sd = sd(TTR))
cohens_d(ttr_means)
```

### Joint Attention
```{r, e2ja-regressions, echo=F, include=F}

# Total number of bids
e2bids_m <- stan_lmer(bids ~ age * Condition + (1|video), data=e2ja)
summary(e2bids_m) # condition mean=3.1, 95%CI = [1.1, 5.0]
p_direction(e2bids_m, effects="all") # age 84%
e2bids <- get_stan_glmer_reporting_values(e2bids_m, "ConditionActivity Video")

e2tja_m <- stan_glmer(total_ja ~ age * Condition + (1|video), data=e2ja)
summary(e2tja_m) # total episodes of joint attention -- age, age*condition, condition
p_direction(e2tja_m, effects="all") # age, cond, age:cond
e2tjaAge = get_stan_glmer_reporting_values(e2tja_m, "age")
e2tjaCond = get_stan_glmer_reporting_values(e2tja_m, "ConditionActivity Video")
e2tjaAgeCond = get_stan_glmer_reporting_values(e2tja_m, "age:ConditionActivity Video")
```

We fit Bayesian mixed-effects linear regressions predicting the rate of parental bids for joint attention and the rate of JA episodes as a function of fixed effects of condition, age (centered), and their interaction, with random intercepts per video.
Shown in Figure \ref{fig:JA} (left bottom), parents made bids for JA at a greater rate after watching the Activity Video (mean: `r round(subset(e2_bids, Condition=="Activity Video")$mean,2)`, 95% conf. int.: [`r round(subset(e2_bids, Condition=="Activity Video")$ci_lower,2)`, `r round(subset(e2_bids, Condition=="Activity Video")$ci_upper,2)`]; $\beta=`r e2bids["ConditionActivity Video"]`$, `r print_CI(e2bids)`) than after the Science Video (mean: `r round(subset(e2_bids, Condition=="Science Video")$mean,2)`, 95% conf. int.: [`r round(subset(e2_bids, Condition=="Science Video")$ci_lower,2)`, `r round(subset(e2_bids, Condition=="Science Video")$ci_upper,2)`]). 
There were no other effects on parental bids for JA.

The regression predicting rate of JA episodes revealed an effect of condition ($\beta=`r e2tjaCond["ConditionActivity Video"]`$, `r print_CI(e2tjaCond)`), with JA episodes occurring at a greater rate after the Activity Video (mean: `r round(subset(e2_tja, Condition=="Activity Video")$mean,2)`, 95% conf. int.: [`r round(subset(e2_tja, Condition=="Activity Video")$ci_lower,2)`, `r round(subset(e2_tja, Condition=="Activity Video")$ci_upper,2)`]) than after the Control Video (mean: `r round(subset(e2_tja, Condition=="Science Video")$mean,2)`, 95% conf. int.: [`r round(subset(e2_tja, Condition=="Science Video")$ci_lower,2)`, `r round(subset(e2_tja, Condition=="Science Video")$ci_upper,2)`]).
Older children also participated in JA episodes at a lesser rate than younger children ($\beta=`r e2tjaAge["age"]`$, `r print_CI(e2tjaAge)`). 
However, this age effect was moderated in the Activity Video condition ($\beta=`r e2tjaAgeCond["age:ConditionActivity Video"]`$, `r print_CI(e2tjaAgeCond)`): shown in Figure \ref{fig:JA} (right bottom), older children did not engage in episodes of JA at a lower rate after an activity video.



```{r e2ja-exploratory, echo=F, include=F}
# EXPLORATORY ANALYSES (NOT PREREGISTERED)

# Episodes of coordinated joint attention
e2cja_m <- stan_lmer(cja ~ age * Condition + (1 | video), data = e2ja)
summary(e2cja_m) #
p_direction(e2cja_m, effects="all") # cond, age:cond, age 90%
e2cjaCond <- get_stan_glmer_reporting_values(e2cja_m, "ConditionActivity Video")
e2cjaAgeCond <- get_stan_glmer_reporting_values(e2cja_m, "age:ConditionActivity Video")


# Episodes of passive joint attention.
e2pja_m <- stan_lmer(pja ~ age * Condition + (1 | video), data = e2ja)
summary(e2pja_m) # age beta=-2.7 89% CI=[-5.1,0.0]
p_direction(e2pja_m, effects="all") # age 
e2pja <- get_stan_glmer_reporting_values(e2pja_m, "age")

# Total duration of passive joint attention.
e2pjat_m <- stan_lmer(pja_length ~ age * Condition + (1|video), data=e2ja)
summary(e2pjat_m) # age: est: -17.97 89% CI = [-33.37,-2.41] 
p_direction(e2pjat_m, effects="all") # age
e2pjat <- get_stan_glmer_reporting_values(e2pjat_m, "age")

# Total duration of coordinated joint attention.
e2cjat_m <- stan_glmer(cja_length ~ age * Condition + (1|video), data=e2ja)
summary(e2cjat_m) 
p_direction(e2cjat_m, effects="all") # age, cond 85%, age:cond 86%
e2cjat <- get_stan_glmer_reporting_values(e2cjat_m, "age")
```

### Exploratory Analyses

Four additional exploratory regressions with a similar structure were carried out to predict the number and duration of coordinated and passive JA episodes.
The regression predicting the rate of coordinated JA episodes found an effect of condition ($\beta=`r e2cjaCond["ConditionActivity Video"]`$, `r print_CI(e2cjaCond)`), with a greater rate of coordinated JA episodes occurring after the Activity Video (mean: `r round(subset(e2_cja, Condition=="Activity Video")$mean,2)`, 95% CI: [`r round(subset(e2_cja, Condition=="Activity Video")$ci_lower,2)`, `r round(subset(e2_cja, Condition=="Activity Video")$ci_upper,2)`]) than after the Control Video (mean: `r round(subset(e2_cja, Condition=="Science Video")$mean,2)`, 95% CI: [`r round(subset(e2_cja, Condition=="Science Video")$ci_lower,2)`, `r round(subset(e2_cja, Condition=="Science Video")$ci_upper,2)`]).
There was an interaction of age and condition ($\beta=`r e2cjaAgeCond["age:ConditionActivity Video"]`$, `r print_CI(e2cjaAgeCond)`), shown in Figure \ref{fig:e2ja-coord}, revealing that after an Activity Video older children participated in coordinated JA episodes at a greater rate than children in the Control Video condition.
The regression predicting the time spent in coordinated JA episodes found no notable effects.
Older children both engaged in episodes of passive JA at a greater rate with their caregiver ($\beta=`r e2pja["age"]`$, `r print_CI(e2pja)`), and spent more time in passive JA with their caregiver ($\beta=`r e2pjat["age"]`$, `r print_CI(e2pjat)`).
Overall, these results show that the older children in our sample engage in more and longer episodes of joint attention with their caregivers, and that activity videos in particular lead to more episodes of coordinated JA.

As for Experiment 1, we conducted a corpus characteristic analysis to examine differences parents' language use in the two conditions ([see SI](https://osf.io/2bpdf/files/github/language_analyses/Exp2_characteristic_chart.html)).
The words that were strongly indicative of being from the Activity Video condition include "big", "little", "give", "small", "cow", "yellow", "take", and "put", while words that were most characteristic of the Science Video condition include "beep", "like", "neigh", "uh-oh", "say", "for", "does", and "did". 

```{r e2ja-coord, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=5.5, fig.height=3.5, fig.cap = "The number of episodes of coordinated JA by condition and age in Experiment 2."}
# Older children in the Activity Video condition engaged in more episodes of coordinated JA with their caregiver than dyads in the Control Video condition. (no interpretation in PsychSci captions)

#Episodes of coordinated joint attention
e2cjad <- e2ja %>% group_by(Condition) %>%
  tidyboot_mean(cja) 

e2cja <- ggplot(e2cjad, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  geom_point(data=e2ja, aes(x=Condition, y=cja), alpha=.2, position = position_jitterdodge()) +
  xlab("Condition") + ylab("Episodes of Coordinated JA") +
  langcog::scale_fill_solarized() + ggthemes::theme_few() + theme(legend.position="none") 

e2cja_age = ggplot(e2ja, aes(x = Age, y = cja, group=Condition, colour = Condition)) +
  geom_point(alpha=.7) + geom_smooth(method='lm') + 
  xlab("Age (years)") + ylab("Episodes of Coordinated JA") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() 

print(e2cja_age)
#multiplot(e2cja, e2tja_age, cols=4, layout=matrix(c(1,2,2,2), nrow=1))
```


# General Discussion

We were interested in how digital parenting advice alters parents' interactions with their children. 
We specifically set out to test whether activity suggestions led to higher-quality play, a presupposition of many early parenting interventions. 
Our experiments explored this question by randomly assigning parents to different advice conditions and then observing their behavior in short free-play sessions, with quality of play assessed through measures of parent language and joint attention. 
In two experiments, we found that activity videos increased the rate of parents' bids for joint attention as compared with no video (Experiment 1) and a comparable science video (Experiment 2). 
In some cases---especially in older children---these bids were successful in increasing engagement. 
We also observed differences in parents' talk that were broadly similar across both experiments, with a greater quantity of language but a similar breadth of vocabulary (leading to lower measures of lexical diversity). 
Exploratory corpus analysis identified the words most characteristic of the activity videos as being related to requests (e.g., "give", "put", "take") whereas control conditions featured more invitational words (e.g., "say", "let", "like"), often asking about animal noises ("What does the X say?").

The short, activity-oriented parenting messages we used encouraged parents to make more attempts---both verbal and non-verbal---to engage their child, supporting their use as a component of interventions. 
Why were they successful? 
When parents are asked to play with their children in the presence of new toys, they may choose to follow their child's lead and engage in free play.
While free play is positive, it nevertheless results in less scaffolded activity than when parents are given a goal that suggests a repertoire of ways to guide their child. 
Parents may also persist in providing opportunities for their child to complete the activity, leading to more repetitive language but also more offers of engagement.

Our study has a number of limitations related to design and sample, each of which suggests possible future directions. 
First, our design was intentionally short and minimal; future studies should investigate whether changes in parents' speech and attempts to engage their children could persist across a longer timespan (perhaps with a broader set of activities being provisioned). 
A longer-term study would also address whether consistent increases in parent bids would lead children to respond by engaging more with their parent.
Second, our design assumes that parents have access to the materials needed to complete the suggested activities; this assumption may be unrealistic for any parent, but especially for the parents who are most likely to be targeted for early parenting interventions. 
Providing materials may be critical for the success of activity suggestions. 
Finally, our sample is a convenience sample drawn from a museum, but it skews towards higher socio-economic status households as well as those families who are well-disposed towards visiting a museum (perhaps because they value education) and are interested in participating in research. 
A key goal for future research is to assess the generality of these findings across populations. 

<!-- Taken together with the greater number of bids for joint attention from parents, we suggest that the activity videos may lead parents to more persistently and repetitively attempt to engage their child in the prescribed activity. -->

<!-- Both experiments also found that activity videos led to lower lexical diversity in parents' child-directed speech (TTR in both; MTLD only in Exp. 1). However, activity videos resulted in parents using more word tokens, while the number of word types did not differ significantly across conditions. -->
<!-- Thus, although activity videos led to lower lexical diversity during play, this resulted not from fewer word types but from the use of more word tokens. -->

In sum, the results of this study show that digital parenting videos recommending play activities can lead to short-term increases in parents' attempts to engage their young children, both verbally and non-verbally. 


# Acknowledgements

This work was supported by a gift and a contract from Kinedu, Inc. to the Language and Cognition Lab. 
Thanks to Samaher Radwan and Megan Merrick for behavioral coding of the videos, and to members of the Language and Cognition Lab at Stanford for helpful discussion.

\newpage

# References
```{r create_r-references}
r_refs(file = "library.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

